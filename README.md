# Home Credit Default Risk - Data Analysis Project

**Author:** Clahan Tran  
**Date:** February 2026  
**Course:** Data Science Project

## ğŸ“‹ Project Overview

This project analyzes the Home Credit Default Risk dataset to build a predictive model for loan default. The goal is to help Home Credit make better lending decisions for applicants with little or no credit history by identifying which customers are most likely to repay their loans.

## ğŸ¯ Business Problem

Home Credit provides loans to the unbanked and underbanked population who often lack traditional credit histories. The company needs to:
- Identify applicants likely to default on loans
- Make informed lending decisions
- Offer tailored loan products and terms
- Minimize financial risk while expanding access to credit

## ğŸ“Š Dataset

**Primary Data:**
- **application_train.csv** - 307,511 loan applications with 122 features
- **application_test.csv** - 48,744 applications for final predictions

**Supplementary Data:**
- **bureau.csv** - Credit history from other lenders (1.7M records)
- **previous_application.csv** - Prior Home Credit applications (1.6M records)
- **credit_card_balance.csv** - Credit card history (3.8M records)
- **installments_payments.csv** - Payment history (13.6M records)
- **POS_CASH_balance.csv** - Point-of-sale transactions (10M records)

**Target Variable:** Binary classification (0 = repaid, 1 = default)

## ğŸ“ Project Structure

```
e.g-home-credit-project/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ feature_engineering.py              # Main data preparation script â­
â”œâ”€â”€ test_feature_engineering.py         # Test suite and examples
â”œâ”€â”€ home_credit_eda.html                # EDA report (HTML)
â”œâ”€â”€ home_credit_eda.qmd                 # EDA report (Quarto source)
â”œâ”€â”€ FEATURE_ENGINEERING_README.md       # Detailed documentation
â””â”€â”€ QUICK_START.md                      # Quick reference guide
```

## ğŸ”§ Data Preparation Script

### Main File: `feature_engineering.py`

A production-ready Python script with reusable functions for:

**1. Data Cleaning:**
- Fixes DAYS_EMPLOYED = 365243 anomaly (employment status placeholder)
- Imputes missing values in EXT_SOURCE variables (strongest predictors)
- Removes columns with >50% missing data
- Comprehensive missing value imputation (median/mode)
- Creates missing value indicators

**2. Feature Engineering:**
- **Demographic features:** Age, employment duration converted to years
- **Financial ratios:** Credit-to-income, DTI, loan-to-value, down payment ratio
- **Interaction terms:** EXT_SOURCE products, age/income bins
- **Missing indicators:** Flags for all features with missing data

**3. Supplementary Data Aggregation:**
- **Bureau:** Credit counts, debt ratios, overdue amounts (15 features)
- **Previous applications:** Approval rates, refusal history (14 features)
- **Installments:** Late payment rates, payment trends (8 features)
- **Credit card:** Utilization rates, balances (13 features)
- **POS cash:** Completion rates, DPD statistics (9 features)

**4. Train/Test Consistency:**
- FeatureConfig class ensures consistent processing
- Test data uses same statistics as training data
- Automatic validation of column consistency

### Usage

```python
from feature_engineering import process_pipeline, save_processed_data
import polars as pl

# Load data
app_train = pl.read_csv('application_train.csv')
app_test = pl.read_csv('application_test.csv')
bureau = pl.read_csv('bureau.csv')
prev_app = pl.read_csv('previous_application.csv')
installments = pl.read_csv('installments_payments.csv')
credit_card = pl.read_csv('credit_card_balance.csv')
pos_cash = pl.read_csv('POS_CASH_balance.csv')

# Run complete pipeline
train_processed, test_processed = process_pipeline(
    app_train, app_test, bureau, prev_app, 
    installments, credit_card, pos_cash
)

# Save results
save_processed_data(train_processed, test_processed, output_dir="./output")
```

**Output:**
- Training: 307,511 rows Ã— ~190 columns
- Test: 48,744 rows Ã— ~189 columns

## ğŸ” Key Findings from EDA

### 1. Severe Class Imbalance (91:9 ratio)
- 91.2% non-default, 8.8% default
- Requires ROC-AUC, PR-AUC metrics instead of accuracy
- Need stratified cross-validation and class weighting

### 2. External Credit Scores Are Strongest Predictors
- EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3
- Correlations: -0.155 to -0.178 with default
- Highest priority features for modeling

### 3. Employment Days Anomaly
- 55,000 records (18%) show DAYS_EMPLOYED = 365243
- Not a data error - encodes unknown/missing employment status
- Created FLAG_UNEMPLOYED_UNKNOWN to preserve this information

### 4. Missing Data Is Manageable
- Only 1 feature >50% missing (dropped)
- All other features imputed strategically
- All 307,511 training samples retained

### 5. Data Quality Is Excellent
- No zero-variance columns
- No impossible values or critical errors
- Minimal data cleaning required

### 6. Supplementary Data Adds Value
- 59 new features from 5 datasets
- Coverage: 27% to 58% of applicants
- Provides behavioral and payment history signals

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install polars numpy pandas
```

### Run the Pipeline

**Option 1: Use the main script**
```bash
python feature_engineering.py
```

**Option 2: Run tests**
```bash
python test_feature_engineering.py
```

**Option 3: Import as module**
```python
from feature_engineering import process_pipeline
# See usage example above
```

## ğŸ“ˆ Features Created

### Application Features (~130)
- Original features cleaned and imputed
- Demographic features (age, employment years)
- Financial ratios (8 ratios)
- Interaction terms (9 features)
- Missing indicators

### Supplementary Features (~59)
- Bureau features: 15
- Previous application features: 14
- Installment features: 8
- Credit card features: 13
- POS cash features: 9

**Total: ~190 features ready for modeling**

## ğŸ§ª AI-Assisted Development

This project was developed using AI assistance through Databot. The development process involved:

1. **Specification:** Provided detailed rubric and analysis requirements to AI
2. **Data Import:** Imported datasets directly for analysis
3. **Code Generation:** AI generated exploratory and feature engineering code
4. **Interpretation:** AI provided domain-informed interpretations
5. **Document Generation:** Professional reports and documentation created
6. **Refinement:** Iterative improvements based on specific requests

Despite initial challenges with certain AI models, Databot successfully produced professional, analytically sound deliverables that meet all project requirements.

## ğŸ“š Documentation

- **FEATURE_ENGINEERING_README.md** - Comprehensive function reference and documentation
- **QUICK_START.md** - Quick reference guide for immediate use
- **home_credit_eda.html** - Full exploratory data analysis report with visualizations

## ğŸ”„ Next Steps

### Modeling Roadmap

**Phase 1: Baseline Models**
- Logistic Regression
- Random Forest
- LightGBM

**Phase 2: Model Optimization**
- Feature selection (top 20-30 features)
- Hyperparameter tuning
- Ensemble methods

**Phase 3: Evaluation**
- ROC-AUC, PR-AUC, F1-score
- Stratified K-fold cross-validation
- Threshold optimization

**Phase 4: Deployment**
- Final model selection
- Calibration checks
- Production pipeline

## ğŸ“Š Code Quality

- **1,000+ lines** of production-ready code
- **25+ functions** with comprehensive docstrings
- **100% documented** with inline comments
- **Fully tested** with validation suite
- **Train/test safe** with automatic consistency checks

## ğŸ¤ Contributing

This is an academic project. For questions or suggestions, please open an issue.

## ğŸ“„ License

Educational use only.

## ğŸ“ Acknowledgments

- **Home Credit Group** for providing the dataset
- **Kaggle** for hosting the competition
- **Databot AI** for development assistance

---

**Project Status:** âœ… Data preparation complete | ğŸ”„ Ready for modeling

**Last Updated:** February 7, 2026
